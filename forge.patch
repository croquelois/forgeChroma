diff --git a/backend/loader.py b/backend/loader.py
index e824f6b..717d368 100644
--- a/backend/loader.py
+++ b/backend/loader.py
@@ -22,6 +22,7 @@ from backend.diffusion_engine.sd20 import StableDiffusion2
 from backend.diffusion_engine.sdxl import StableDiffusionXL, StableDiffusionXLRefiner
 from backend.diffusion_engine.sd35 import StableDiffusion3
 from backend.diffusion_engine.flux import Flux
+from backend.diffusion_engine.chroma import Chroma
 
 
 possible_models = [StableDiffusion, StableDiffusion2, StableDiffusionXLRefiner, StableDiffusionXL, StableDiffusion3, Flux]
@@ -108,7 +109,7 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
             load_state_dict(model, state_dict, log_name=cls_name, ignore_errors=['transformer.encoder.embed_tokens.weight', 'logit_scale'])
 
             return model
-        if cls_name in ['UNet2DConditionModel', 'FluxTransformer2DModel', 'SD3Transformer2DModel']:
+        if cls_name in ['UNet2DConditionModel', 'FluxTransformer2DModel', 'SD3Transformer2DModel', 'ChromaTransformer2DModel']:
             assert isinstance(state_dict, dict) and len(state_dict) > 16, 'You do not have model state dict!'
 
             model_loader = None
@@ -117,6 +118,9 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
             elif cls_name == 'FluxTransformer2DModel':
                 from backend.nn.flux import IntegratedFluxTransformer2DModel
                 model_loader = lambda c: IntegratedFluxTransformer2DModel(**c)
+            elif cls_name == 'ChromaTransformer2DModel':
+                from backend.nn.chroma import IntegratedChromaTransformer2DModel
+                model_loader = lambda c: IntegratedChromaTransformer2DModel(**c)
             elif cls_name == 'SD3Transformer2DModel':
                 from backend.nn.mmditx import MMDiTX
                 model_loader = lambda c: MMDiTX(**c)
@@ -151,7 +155,6 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
                 initial_device = memory_management.unet_inital_load_device(parameters=state_dict_parameters, dtype=storage_dtype)
                 need_manual_cast = storage_dtype != computation_dtype
                 to_args = dict(device=initial_device, dtype=storage_dtype)
-
                 with using_forge_operations(**to_args, manual_cast_enabled=need_manual_cast):
                     model = model_loader(unet_config).to(**to_args)
 
@@ -478,6 +481,14 @@ def split_state_dict(sd, additional_state_dicts: list = None):
 
     return state_dict, guess
 
+class GuessChroma:
+    huggingface_repo = 'Chroma'
+    unet_extra_config = {
+        'guidance_out_dim': 3072,
+        'guidance_hidden_dim': 5120,
+        'guidance_n_layers': 5
+    }
+    unet_remove_config = ['guidance_embed']
 
 @torch.inference_mode()
 def forge_loader(sd, additional_state_dicts=None):
@@ -486,6 +497,17 @@ def forge_loader(sd, additional_state_dicts=None):
     except:
         raise ValueError('Failed to recognize model type!')
     
+    if estimated_config.huggingface_repo == "black-forest-labs/FLUX.1-schnell"  \
+        and "transformer" in state_dicts \
+        and "distilled_guidance_layer.layers.0.in_layer.bias" in state_dicts["transformer"]:
+        estimated_config.huggingface_repo = GuessChroma.huggingface_repo
+        for x in GuessChroma.unet_extra_config:
+            estimated_config.unet_config[x] = GuessChroma.unet_extra_config[x]
+        for x in GuessChroma.unet_remove_config:
+            del estimated_config.unet_config[x]
+        state_dicts['text_encoder'] = state_dicts['text_encoder_2']
+        del state_dicts['text_encoder_2']
+
     repo_name = estimated_config.huggingface_repo
 
     local_path = os.path.join(dir_path, 'huggingface', repo_name)
@@ -540,6 +562,10 @@ def forge_loader(sd, additional_state_dicts=None):
         else:
             huggingface_components['scheduler'].config.prediction_type = prediction_types.get(estimated_config.model_type.name, huggingface_components['scheduler'].config.prediction_type)
 
+    if estimated_config.huggingface_repo == "Chroma":
+        print("load Chroma model")
+        return Chroma(estimated_config=estimated_config, huggingface_components=huggingface_components)
+
     for M in possible_models:
         if any(isinstance(estimated_config, x) for x in M.matched_guesses):
             return M(estimated_config=estimated_config, huggingface_components=huggingface_components)
diff --git a/backend/sampling/condition.py b/backend/sampling/condition.py
index b594d1b..f9fab0e 100644
--- a/backend/sampling/condition.py
+++ b/backend/sampling/condition.py
@@ -102,17 +102,18 @@ def compile_conditions(cond):
         return [result, ]
 
     cross_attn = cond['crossattn']
-    pooled_output = cond['vector']
 
     result = dict(
         cross_attn=cross_attn,
-        pooled_output=pooled_output,
         model_conds=dict(
-            c_crossattn=ConditionCrossAttn(cross_attn),
-            y=Condition(pooled_output)
+            c_crossattn=ConditionCrossAttn(cross_attn)
         )
     )
 
+    if 'vector' in cond:
+        result['pooled_output'] = cond['vector']
+        result['model_conds']['y'] = Condition(cond['vector'])
+
     if 'guidance' in cond:
         result['model_conds']['guidance'] = Condition(cond['guidance'])
 
