diff --git a/backend/loader.py b/backend/loader.py
index e824f6b..717d368 100644
--- a/backend/loader.py
+++ b/backend/loader.py
@@ -22,6 +22,7 @@ from backend.diffusion_engine.sd20 import StableDiffusion2
 from backend.diffusion_engine.sdxl import StableDiffusionXL, StableDiffusionXLRefiner
 from backend.diffusion_engine.sd35 import StableDiffusion3
 from backend.diffusion_engine.flux import Flux
+from backend.diffusion_engine.chroma import Chroma
 
 
 possible_models = [StableDiffusion, StableDiffusion2, StableDiffusionXLRefiner, StableDiffusionXL, StableDiffusion3, Flux]
@@ -108,7 +109,7 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
             load_state_dict(model, state_dict, log_name=cls_name, ignore_errors=['transformer.encoder.embed_tokens.weight', 'logit_scale'])
 
             return model
-        if cls_name in ['UNet2DConditionModel', 'FluxTransformer2DModel', 'SD3Transformer2DModel']:
+        if cls_name in ['UNet2DConditionModel', 'FluxTransformer2DModel', 'SD3Transformer2DModel', 'ChromaTransformer2DModel']:
             assert isinstance(state_dict, dict) and len(state_dict) > 16, 'You do not have model state dict!'
 
             model_loader = None
@@ -117,6 +118,9 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
             elif cls_name == 'FluxTransformer2DModel':
                 from backend.nn.flux import IntegratedFluxTransformer2DModel
                 model_loader = lambda c: IntegratedFluxTransformer2DModel(**c)
+            elif cls_name == 'ChromaTransformer2DModel':
+                from backend.nn.chroma import IntegratedChromaTransformer2DModel
+                model_loader = lambda c: IntegratedChromaTransformer2DModel(**c)
             elif cls_name == 'SD3Transformer2DModel':
                 from backend.nn.mmditx import MMDiTX
                 model_loader = lambda c: MMDiTX(**c)
@@ -151,7 +155,6 @@ def load_huggingface_component(guess, component_name, lib_name, cls_name, repo_p
                 initial_device = memory_management.unet_inital_load_device(parameters=state_dict_parameters, dtype=storage_dtype)
                 need_manual_cast = storage_dtype != computation_dtype
                 to_args = dict(device=initial_device, dtype=storage_dtype)
-
                 with using_forge_operations(**to_args, manual_cast_enabled=need_manual_cast):
                     model = model_loader(unet_config).to(**to_args)
 
@@ -478,6 +481,14 @@ def split_state_dict(sd, additional_state_dicts: list = None):
 
     return state_dict, guess
 
+class GuessChroma:
+    huggingface_repo = 'Chroma'
+    unet_extra_config = {
+        'guidance_out_dim': 3072,
+        'guidance_hidden_dim': 5120,
+        'guidance_n_layers': 5
+    }
+    unet_remove_config = ['guidance_embed']
 
 @torch.inference_mode()
 def forge_loader(sd, additional_state_dicts=None):
@@ -486,6 +497,17 @@ def forge_loader(sd, additional_state_dicts=None):
     except:
         raise ValueError('Failed to recognize model type!')
     
+    if estimated_config.huggingface_repo == "black-forest-labs/FLUX.1-schnell"  \
+        and "transformer" in state_dicts \
+        and "distilled_guidance_layer.layers.0.in_layer.bias" in state_dicts["transformer"]:
+        estimated_config.huggingface_repo = GuessChroma.huggingface_repo
+        for x in GuessChroma.unet_extra_config:
+            estimated_config.unet_config[x] = GuessChroma.unet_extra_config[x]
+        for x in GuessChroma.unet_remove_config:
+            del estimated_config.unet_config[x]
+        state_dicts['text_encoder'] = state_dicts['text_encoder_2']
+        del state_dicts['text_encoder_2']
+
     repo_name = estimated_config.huggingface_repo
 
     local_path = os.path.join(dir_path, 'huggingface', repo_name)
@@ -540,6 +562,10 @@ def forge_loader(sd, additional_state_dicts=None):
         else:
             huggingface_components['scheduler'].config.prediction_type = prediction_types.get(estimated_config.model_type.name, huggingface_components['scheduler'].config.prediction_type)
 
+    if estimated_config.huggingface_repo == "Chroma":
+        print("load Chroma model")
+        return Chroma(estimated_config=estimated_config, huggingface_components=huggingface_components)
+
     for M in possible_models:
         if any(isinstance(estimated_config, x) for x in M.matched_guesses):
             return M(estimated_config=estimated_config, huggingface_components=huggingface_components)
diff --git a/backend/operations.py b/backend/operations.py
index 5200093..ed7a4dd 100644
--- a/backend/operations.py
+++ b/backend/operations.py
@@ -12,6 +12,7 @@ stash = {}
 
 
 def get_weight_and_bias(layer, weight_args=None, bias_args=None, weight_fn=None, bias_fn=None):
+    scale_weight = getattr(layer, 'scale_weight', None)
     patches = getattr(layer, 'forge_online_loras', None)
     weight_patches, bias_patches = None, None
 
@@ -32,6 +33,8 @@ def get_weight_and_bias(layer, weight_args=None, bias_args=None, weight_fn=None,
             weight = weight_fn(weight)
         if weight_args is not None:
             weight = weight.to(**weight_args)
+        if scale_weight is not None:
+            weight = weight*scale_weight.to(device=weight.device, dtype=weight.dtype)
         if weight_patches is not None:
             weight = merge_lora_to_weight(patches=weight_patches, weight=weight, key="online weight lora", computation_dtype=weight.dtype)
 
@@ -127,6 +130,7 @@ class ForgeOperations:
             self.out_features = out_features
             self.dummy = torch.nn.Parameter(torch.empty(1, device=current_device, dtype=current_dtype))
             self.weight = None
+            self.scale_weight = None
             self.bias = None
             self.parameters_manual_cast = current_manual_cast_enabled
 
@@ -134,6 +138,8 @@ class ForgeOperations:
             if hasattr(self, 'dummy'):
                 if prefix + 'weight' in state_dict:
                     self.weight = torch.nn.Parameter(state_dict[prefix + 'weight'].to(self.dummy))
+                if prefix + 'scale_weight' in state_dict:
+                    self.scale_weight = torch.nn.Parameter(state_dict[prefix + 'scale_weight'])
                 if prefix + 'bias' in state_dict:
                     self.bias = torch.nn.Parameter(state_dict[prefix + 'bias'].to(self.dummy))
                 del self.dummy
diff --git a/backend/sampling/condition.py b/backend/sampling/condition.py
index b594d1b..f9fab0e 100644
--- a/backend/sampling/condition.py
+++ b/backend/sampling/condition.py
@@ -102,17 +102,18 @@ def compile_conditions(cond):
         return [result, ]
 
     cross_attn = cond['crossattn']
-    pooled_output = cond['vector']
 
     result = dict(
         cross_attn=cross_attn,
-        pooled_output=pooled_output,
         model_conds=dict(
-            c_crossattn=ConditionCrossAttn(cross_attn),
-            y=Condition(pooled_output)
+            c_crossattn=ConditionCrossAttn(cross_attn)
         )
     )
 
+    if 'vector' in cond:
+        result['pooled_output'] = cond['vector']
+        result['model_conds']['y'] = Condition(cond['vector'])
+
     if 'guidance' in cond:
         result['model_conds']['guidance'] = Condition(cond['guidance'])
 
diff --git a/backend/text_processing/t5_engine.py b/backend/text_processing/t5_engine.py
index e00cccc..e235c1f 100644
--- a/backend/text_processing/t5_engine.py
+++ b/backend/text_processing/t5_engine.py
@@ -17,7 +17,7 @@ class PromptChunk:
 
 
 class T5TextProcessingEngine:
-    def __init__(self, text_encoder, tokenizer, emphasis_name="Original", min_length=256):
+    def __init__(self, text_encoder, tokenizer, emphasis_name="Original", min_length=256, end_with_pad=False):
         super().__init__()
 
         self.text_encoder = text_encoder.transformer
@@ -25,6 +25,7 @@ class T5TextProcessingEngine:
 
         self.emphasis = emphasis.get_current_option(opts.emphasis)()
         self.min_length = min_length
+        self.end_with_pad = end_with_pad
         self.id_end = 1
         self.id_pad = 0
 
@@ -80,6 +81,9 @@ class T5TextProcessingEngine:
 
             chunk.tokens = chunk.tokens + [self.id_end]
             chunk.multipliers = chunk.multipliers + [1.0]
+            if self.end_with_pad:
+                chunk.tokens = chunk.tokens + [self.id_pad]
+                chunk.multipliers = chunk.multipliers + [1.0]
             current_chunk_length = len(chunk.tokens)
 
             token_count += current_chunk_length
